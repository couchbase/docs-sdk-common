= Document
:nav-title: Documents & Doc Ops
include::partial$attributes.adoc[]
:page-topic-type: concept
:page-aliases: ROOT:core-operations,ROOT:documents,ROOT:documents-basics,ROOT:documents-atomic

[abstract]
Couchbase supports CRUD operations, various data structures, and binary documents.
////
At the core of a database is its ability to store and retrieve data.
This section discusses the various ways in which you can access data (Documents) in Couchbase: creating, updating, retrieving, querying, and deleting documents.
////
//??????

// tag::document[]
== Document

A _document_ refers to an entry in the database (other databases may refer to the same concept as a xref:7.0@server:learn:data/document-data-model.adoc#couchbase-server-and-json-the-benefits[_row_]).
A document has an ID (_primary key_ in other databases), which is unique to the document and by which it can be located.
The document also has a value which contains the actual application data.

*Document IDs* (keys) are assigned by application.
A valid document ID must:

* Conform to UTF-8 encoding
* Be no longer than 250 bytes
+
NOTE: There is a difference between bytes and characters: most non-Latin characters occupy more than a single byte.

You are free to choose any ID (key) for your document, so long as it conforms to the above restrictions.
Unlike some other database, Couchbase does not automatically generate IDs for you, though you may use a separate xref:#counters[counter] to increment a serial number -- you can also use UUIDs as keys, the best choice being determined by your use case.

The *document value* contains the actual application data; for example, a _product_ document may contain information about the price and description.
Documents are usually (xref:nonjson.adoc[but not always]) stored as JSON on the server.
Because JSON is a structured format, it can be subsequently searched and queried.

[source,json]
----
{
    "type": "product",
    "sku": "CBSRV45DP",
    "msrp": [5.49, "USD"],
    "ctime": "092011",
    "mfg": "couchbase",
    "tags": ["server", "database", "couchbase", "nosql", "fast", "json", "awesome"]
}
----
// end::document[]

// tag::crud-overview[]
In Couchbase documents are stored using one of the operations: `upsert`, `insert`, and `replace`.
Each of these operations will write a JSON document with a given document ID (key) to the database.
The update methods differ in behavior in respect to the existing state of the document:

* `insert` will only create the document if the given ID is not found within the database.
* `replace` will only replace the document if the given ID already exists within the database.
* `upsert` will always replace the document, ignoring whether the ID already exists or not.

Documents can be retrieved using the `get` operation, and finally removed using the `remove` operations.

Since Couchbase’s KV store may be thought of as a distributed hashmap or dictionary, the following code samples are explanatory of Couchbase’ update operations in pseudo-code:

[source,cpp]
----
map<string,object> KV_STORE;

void insert(string doc_id, object value) {
    if (!KV_STORE.contains(doc_id)) {
        KV_STORE.put(doc_id, value);
    } else {
        throw DocumentAlreadyExists();
    }
}

void replace(string doc_id, object value) {
    if (KV_STORE.contains(doc_id)) {
        KV_STORE.put(doc_id, value);
    } else {
        throw DocumentNotFound();
    }
}

void upsert(string doc_id, object value) {
    KV_STORE.put(doc_id, value);
}

object get(string doc_id) {
    if (KV_STORE.contains(doc_id)) {
        return KV_STORE.get(doc_id);
    } else {
        throw DocumentNotFound();
    }
}
----

You can also use xref:n1ql-query.adoc[N1QL Queries] and xref:full-text-search-overview.adoc[Full Text Search] to access documents by means other than their IDs, however these query operations Couchbase eventually translate into primitive key-value operations, and exist as separate services outside the data store.
// end::crud-overview[]

// tag::store-update[]
== Storing and Updating Documents

Documents can be stored and updated using either the SDK, Command line, or Web UI.
When using a storage operation, the _full content_ of the document is replaced with a new value.

The following example shows a document being stored using the xref:webui-cli-access.adoc#cli-access[`cbc`] utility.
The ID of the document is `docid` and its value is JSON containing a single field (`json`) with the value of `value`.

[source,console]
----
# When storing JSON data using cbc, ensure it is properly quoted for your shell:
$ cbc create -u Administrator -P password docid -V '{"json":"value"}' -M upsert -U couchbase://cluster-node/bucket-name
docid               Stored. CAS=0x8234c3c0f213
----

You can also specify additional options when storing a document in Couchbase

* xref:#expiry[Expiry] (or `TTL`) value which will instruct the server to delete the document after a given amount of time.
This option is useful for transient data (such as sessions).
By default documents do not expire.
See xref:#expiry[Expiry] for more information on expiration.
// * xref:howtos:concurrent-mutations-cluster.adoc[CAS]
* CAS value to protect against concurrent updates to the same document.
// See xref:concurrent-mutations-cluster.adoc[CAS] for a description on how to use CAS values in your application.
* xref:durability-replication-failure-considerations.adoc[Durability Requirements]
// end::store-update[]

[NOTE]
====
If you wish to only modify certain parts of a document, you can use xref:subdocument-operations.adoc[sub-document] operations which operate on specific subsets of documents:

[source,python]
----
cb.mutate_in('docid', subdoc.array_addunique('tags', 'fast'))
----

or xref:7.0@server:n1ql:n1ql-language-reference/update.adoc[N1QL UPDATE] to update documents based on specific query criteria:

[source,n1ql]
----
update `default` SET sale_price = msrp * 0.75 WHERE msrp < 19.95;
----
====

// tag::get_generic[]
== Retrieving Documents

This section discusses retrieving documents using their IDs, or primary keys.
Documents can also be accessed using secondary lookups via xref:n1ql-query.adoc[N1QL queries] and xref:howtos:view-queries-with-sdk.adoc[MapReduce Views].
Primary key lookups are performed using the key-value API, which simplifies use and increases performance (as applications may interact with the KV store directly, rather than a secondary index or query processor).

In Couchbase, documents are stored with their IDs.
Retrieving a document via its ID is the simplest and quickest operation in Couchbase.

....
>>> result = cb.get('docid')
>>> print result.value
{'json': 'value'}
....

[source,console]
----
$ cbc cat docid
docid                CAS=0x8234c3c0f213, Flags=0x0. Size=16
{"json":"value"}
----

Once a document is retrieved, it is accessible in the native format by which it was stored; meaning that if you stored the document as a list, it is now available as a list again.
The SDK will automatically deserialize the document from its stored format (usually JSON) to a native language type.
It is possible to store and retrieve non-JSON documents as well, using a xref:nonjson.adoc[transcoder].

You can also modify a document's expiration time while retrieving it; this is known as _get-and-touch_ and allows you to keep temporary data alive while retrieving it in one atomic and efficient operation.

Documents can also be retrieved with N1QL.
While N1QL is generally used for secondary queries, it can also be used to retrieve documents by their primary keys (ID) (though it is recommended to use the key-value API if the ID is known).
Lookups may be done either by comparing the `META(from-term).id` or by using the `USE KEYS` [\...] keyword:
// end::get_generic[]

[source,n1ql]
----
SELECT * FROM default USE KEYS ["docid"];
----

or

[source,n1ql]
----
SELECT * FROM default WHERE META(default).id = "docid";
----

You can also retrieve _parts_ of documents using xref:subdocument-operations.adoc[sub-document operations], by specifying one or more sections of the document to be retrieved

[source,python]
----
name, email = cb.retrieve_in('user:kingarthur', 'contact.name', 'contact.email')
----

// tag::counters1[]
== Counters

You can atomically increment or decrement the numerical value of special counter document -- examples can be found in the xref:howtos:kv-operations.adoc#atomic-counters[practical K-V Howto document].

CAUTION: Do not increment or decrement counters if using XDCR.
Within a single cluster the `incr()` is atomic, as is `decr()`;
across XDCR however, if two clients connecting to two different (bidirectional) clusters issue `incr` concurrently,
this may (and most likely will) result in the value only getting incremented once in total.
The same is the case for `decr()`.

A document may be used as a counter if its value is a simple ASCII number, like `42`.
Couchbase allows you to increment and decrement these values atomically using a special `counter` operation in the `Binary.Collection`.
The example below shows a counter being initialised, then being incremented and decremented:
// end::counters1[]

[source,java]
----
String counterDocId = "counter-doc";
// Increment by 1, creating doc if needed
collection.binary().increment(counterDocId);
// Decrement by 1
collection.binary().decrement(counterDocId);
// Decrement by 5
collection.binary().decrement(counterDocId,
DecrementOptions.decrementOptions().delta(5));
----

// tag::counters2[]
In the above example, a counter is created by using the `counter` method with an `initial` value.
The initial value is the value the counter uses if the counter ID does not yet exist.

Once created, the counter can be incremented or decremented atomically by a given _amount_ or _delta_.
Specifying a positive delta increments the value and specifying a negative one decrements it.
When a counter operation is complete, the application receives the current value of the counter, after the increment.

Couchbase counters are 64-bit unsigned integers in Couchbase and do not wrap around if decremented beyond 0.
However, counters will wrap around if incremented past their maximum value (which is the maximum value contained within a 64-bit integer).
Many SDKs will limit the _delta_ argument to the value of a _signed_ 64-bit integer.

<<expiry,Expiration>> times can also be specified when using counter operations.

xref:howtos:concurrent-document-mutations.adoc[CAS] values are not used with counter operations since counter operations are atomic.
The intent of the counter operation is to simply increment the current server-side value of the document.
If you wish to only increment the document if it is at a certain value, then you may use a normal `upsert` function with CAS:
// end::counters2[]

[source,python]
----
rv = cb.get('counter_id')
value, cas = rv.value, rv.cas
if should_increment_value(value):
  cb.upsert('counter_id', value + increment_amount, cas=cas)
----

// tag::counters3[]
You can also use xref:subdocument-operations.adoc[sub-document counter operations] to increment numeric values _within_ a document containing other content.
An example can be found in the xref:howtos:subdocument-operations.adoc#counters-and-numeric-fields[practical sub-doc page].
// end::counters3[]


[#devguide_datastructures]
== Datastructures - List, Map, Set, Queue

You can use collection data structures such as lists, maps, sets and queues in Couchbase.
These data structures may be manipulated with basic operations without retrieving and storing the entire document.

See the data structures xref:python-sdk::datastructures.adoc[documentation for each SDK language] for details on implementation.
Some Python examples are provided below for easy reference, but more advanced Collections frameworks are accessible in xref:java-sdk::datastructures.adoc[Java] and xref:dotnet-sdk::datastructures.adoc[.NET] as well.

Data structures in Couchbase are similar in concept to data structures in, for example, Python:

* *Map* is like Python `dict`, and is a key-value structure, where a value is accessed by using a key string.
* *List* is like a Python `list` and is a sequential data structure.
Values can be placed in the beginning or end of a list, and can be accessed using numeric indexes.
* *Queue* is a wrapper over a _list_ which offers FIFO (first-in-first-out) semantics, allowing it to be used as a lightweight job queue.
* *Set* is a wrapper over a _list_ which provides the ability to handle unique values.

These data structures are stored as JSON documents in Couchbase, and can therefore be accessed using N1QL, Full Text Search, and normal key-value operations.
Data structures can also be manipulated using the traditional sub-document and full-document key-value APIs.

To add an item to a map, specify the _document ID_ of the map itself (i.e.
the ID which uniquely identifies the map in the server), the key _within_ the map, and the value to store under the key:

[source,python]
----
bucket.map_add('map_id', 'name', 'Mark Nunberg', create=True)
----

Data structures can be accessed using their appropriate methods.
Most data access methods will return an [.api]`ValueResult`-like object with the actual returned value under the [.var]`value` property.

[source,python]
----
bucket.list_get(0).value  # 'hello'
bucket.map_get('map_id', 'name').value  # 'mark nunberg'
----

[#devguide_kvcore_append_prepend_generic]
== Raw Byte Concatenation

[#messagepanel_xx2_btg_vt]
[WARNING]
====
The following methods should not be used with JSON documents.

The append and prepend operations operate at the byte level and are unsuitable for dealing with JSON documents.
Use these methods only when explicitly dealing with binary or UTF-8 documents.
Using the append and prepend methods may invalidate an existing JSON document.
You can use xref:subdocument-operations.adoc[sub-document operations] if you want to have true JSON-aware prepend and append operations which add values to JSON arrays.
====

[source,python]
----
append(docid, fragment)
prepend(docid, fragment)
----

The _append_ and _prepend_ operations atomically add bytes to the end or beginning of a binary document.
They are an efficient alternative to retrieving a binary document in its entirety, appending the contents locally, and then saving the contents back to the server.

Because these methods do raw string manipulation, they are only suitable for non-JSON documents: Prepending or appending anything to a JSON document will invalidate the JSON and make it unparseable by standard JSON parsers.

The semantics of the _append_ and _prepend_ operations are similar to those of the _upsert_ family of operations, except that they accept the fragment to append as their value, rather than the entire document.
These functions may be used to add efficiency for custom binary data structures (such as logs), as they avoid transferring the contents of the entire document for each operation.
Consider the following versions (which are equivalent)

.Append using get() and replace() (slow)
[source,python]
----
# Store the document
cb.upsert('binary_doc', '\x01', format=couchbase.FMT_BYTES)

while True:
    # Retrieve the entire document
    rv = cb.get('binary_doc')
    value = rv.value + '\x02'
    try:
        # Upload the entire document
        cb.replace('binary_doc', value, format=couchbase.FMT_BYTES)
        break
    except couchbase.exceptions.KeyExistsError:
        continue

print repr(cb.get('binary_doc').value)
----

.Append using append() (fast)
[source,python]
----
# Store the document
cb.upsert('binary_doc', '\x01', format=couchbase.FMT_BYTES)

# Append a fragment
cb.append('binary_doc', '\x02', format=couchbase.FMT_BYTES)

print repr(cb.get('binary_doc').value)
----

Note that since the _append_ operation is done atomically, there is no need for a CAS check (though one can still be supplied if the document must be at a specific state).

Users of the _append_ and _prepend_ operations should ensure that the resulting documents do not become too large.
Couchbase has a hard document size limit of 20MB.

Using _append_ and _prepend_ on larger documents may cause performance degradation and memory fragmentation at the server level, as for each append operation the server must allocate memory for the new document size and then append the fragment to the new memory.
The performance impact may be significant when document sizes reach beyond 100KB.

Finally, note that while append saves network traffic from the client to server (by only specifying the fragment to append), the entire document is replicated for each mutation.
Five append operations on a single 10MB document will result in 50MB of traffic to each replica.

// tag::expiration[]
[#expiry]
== Expiration Overview

Most data in a database is there to be persisted and long-lived.
However, the need for transient or temporary data does arise in applications, such as in the case of user sessions, caches, or temporary documents representing a given process ownership.
You can use expiration values on documents to handle transient data.

In databases without a built-in expiration feature, dealing with transient data may be cumbersome.
To provide "expiration" semantics, applications are forced to record a time stamp in a record, and then upon each access of the record check the time stamp and, if invalid, delete it.

Since some logically ‘expired’ documents might never be accessed by the application, to ensure that temporary records do not persist and occupy storage, a scheduled process is typically also employed to scan the database for expired entries routinely, and to purge those entries that are no longer valid.

Workarounds such as those described above are not required for Couchbase, as it allows applications to declare the lifetime of a given document, eliminating the need to embed "validity" information in documents and eliminating the need for a routine "purge" of logically expired data.

When an application attempts to access a document which has already expired, the server will indicate to the client that the item is not found.
The server internally handles the process of determining the validity of the document and removing older, expired documents.

== Setting Document Expiration

By default, Couchbase documents do not expire.
However, the expiration value may be set for the _upsert_, _replace_, and _insert_ operations when modifying data.

Couchbase offers two additional operations for setting the document's expiration without modifying its contents:

* The _get-and-touch_ operation allows an application to retrieve a document while modifying its expiration time.
This method is useful when reading session data from the database: since accessing the data is indicative of it still being "alive", _get-and-touch_ provides a natural way to extend its lifetime.
* The _touch_ operation allows an application to modify a document’s expiration time without otherwise accessing the document.
This method is useful when an application is handling a user session but does not need to access the database (for example, if a particular  document is already cached locally).

For Couchbase SDKs which accept simple integer expiry values (as opposed to a proper date or time object) allow expiration to be specified in two flavors.

. As an offset from the current time.
. As an absolute Unix time stamp

If the absolute value of the expiry is less than 30 days (`60 * 60 * 24 * 30` seconds), it is considered an _offset_.
If the value is greater, it is considered an _absolute time stamp_.

It might be preferable for applications to normalize the expiration value, such as by always converting it to an absolute time stamp.
The conversion is performed to avoid issues when the intended offset is larger than 30 days, in which case it is taken to mean a Unix time stamp and, as a result,  the document will expire automatically as soon as it is stored.

[IMPORTANT,caption=Remember]
====
* If you wish to use the expiration feature, then you should supply the expiry value for every mutation operation.
* When dealing with expiration, it is important to note that _most operations will implicitly remove any existing expiration_.
Thus, when modifying a document with expiration, it is important to pass the desired expiration time.
* A document is expired as soon as the current time on the Couchbase Server node responsible for the document exceeds the expiration value.
Bear this in mind in situations where the time on your application servers differs from the time on your Couchbase Server nodes.
====

Note that expired documents are not deleted from the server as soon as they expire.
While a request to the server for an expired document will receive a response indicating the document does not exist, expired documents are actually deleted
(_i.e._ cease to occupy storage and RAM) when an _expiry pager_ is run.
The _expiry pager_ is a routine internal process which scans the database for items which have expired and promptly removes them from storage.

When gathering resource usage statistics, note that expired-but-not-purged items (such as the expiry pager has not scanned this item yet) will still be considered with respect to the overall storage size and item count.

NOTE: Although the API only sets expiry values _per document_, it is possible that elsewhere in the server, an expiry value is being set for xref:7.0@server:learn:buckets-memory-and-storage/expiration.adoc[every document in a bucket^].
Should this be the case, the document TTL may be reduced, and the document may become unavailable to the app sooner than expected.
// end::expiration[]


// tag::exp-note[]
NOTE: If the absolute value of the expiry is less than 30 days (such as `60 * 60 * 24 * 30`), it is considered an _offset_.
If the value is greater, it is considered an _absolute time stamp_.
For more on expiration see the xref:concept-docs:documents.adoc#setting-document-expiration[expiration section] of our documents discussion doc.
// end::exp-note[]




//





= CRUD Document Operations Using the Java SDK with Couchbase Server
:navtitle: Document Operations
:page-topic-type: concept
:page-aliases: documents-creating,documents-updating,documents-retrieving,documents-deleting,howtos:kv-operations

[abstract]
You can access documents in Couchbase using methods of the [.api]`couchbase.couchbase.client.java.Bucket` object.

The methods for retrieving documents are [.api]`get()` and [.api]`lookupIn()` and the methods for mutating documents are [.api]`upsert()`, [.api]`insert()`, [.api]`replace()` and [.api]`mutateIn()`.

Examples are shown using the synchronous API.
See the section on xref:async-programming.adoc[Async Programming] for other APIs.

[#java-additional-options]
== Additional Options

Update operations also accept a xref:core-operations.adoc#expiry[TTL (expiry)] value ([.param]`expiry`) on the passed document which will instruct the server to delete the document after a given amount of time.
This option is useful for transient data (such as sessions).
By default documents do not expire.
See xref:core-operations.adoc#expiry[Expiration Overview] for more information on expiration.

Update operations can also accept a xref:concurrent-mutations-cluster.adoc[CAS] ([.param]`cas`) value on the passed document to protect against concurrent updates to the same document.
See xref:concurrent-mutations-cluster.adoc[CAS] for a description on how to use CAS values in your application.
Since CAS values are opaque, they are normally retreived when a Document is loaded from Couchbase and then used subsequently (without modification) on the mutation operations.
If a mutation did succeed, the returned Document will contain the new CAS value.

[#java-mutation-input]
== Document Input and Output Types

Couchbase stores documents.
From an SDK point of view, those documents contain the actual value (like a JSON object) and associated metadata.
Every document in the Java SDK contains the following properties, some of them optional depending on the context:

|===
| Name | Description

| `id`
| The (per bucket) unique identifier of the document.

| `content`
| The actual content of the document.

| `cas`
| The CAS (Compare And Swap) value of the document.

| `expiry`
| The expiration time of the document.

| `mutationToken`
| The optional MutationToken after a mutation.
|===

There are a few different implementations of a `Document`.
Here are a few noteworthy document types:

* [.api]`JsonDocument`: The default one in most methods, contains a JSON object (as a `JsonObject`).
* [.api]`RawJsonDocument`: Represents any JSON value, stored as a `String` (useful for when you have your own JSON serializer/deserializer).
* [.api]`BinaryDocument`: Used to store pure raw binary data (as a `ByteBuf` from Netty).
+
IMPORTANT: The `ByteBuf` comes from Netty, and when reading one from the SDK, you need to manage its memory by hand by calling `release()`.
See <<java-binary-document,the section about binary documents>>.

Because Couchbase Server can store anything and not just JSON files, many document types exist to satisfy the general needs of an application.
You can also write your own `Document` implementations, which is not covered in this introduction.

[#java-creating-updating-full-docs]
== Creating and Updating Full Documents

Documents may be created and updated using the [.api]`Bucket#upsert()`, [.api]`Bucket#insert()`, and [.api]`Bucket#replace()` family of methods.
Read more about the difference between these methods at xref:core-operations.adoc#crud-overview[Primitive Key-Value Operations] in the Couchbase developer guide.

These methods accept a Document instance where the following values are considered if set:

* [.param]`id` (mandatory): The ID of the document to modify (String).
* [.param]`content` (mandatory): The desired new content of the document, this varies per document type used.
If the `JsonDocument` is used, the document type is a `JsonObject`.
* [.param]`expiry` (optional): Specify the expiry time for the document.
If specified, the document will expire and no longer exist after the given number of seconds.
See xref:core-operations.adoc#expiry[Expiration Overview] for more information.
* [.param]`cas` (optional): The CAS value for the document.
If the CAS on the server does not match the CAS supplied to the method, the operation will fail with a [.api]`CASMismatchException`.
See xref:concurrent-mutations-cluster.adoc[Concurrent Document Mutations] for more information on the usage of CAS values.

Other optional arguments are also available for more advanced usage:

* [.param]`persistTo`, [.param]`replicateTo`: Specify xref:durability.adoc[durability requirements] for the operations.
* [.param]`timeout`, [.param]`timeUnit`: Specify a custom timeout which overrides the default timeout setting.

Upon success, the returned [.api]`Document` instance will contain the new xref:concurrent-mutations-cluster.adoc[CAS] value of the document.
If the document is not mutated successfully, an exception is raised depending on the type of error.

Inserting a document works like this:

[source,java]
----
JsonDocument doc = JsonDocument.create("document_id", JsonObject.create().put("some", "value"));
System.out.println(bucket.insert(doc));
----

....
Output: JsonDocument{id='document_id', cas=216109389250560, expiry=0, content={"some":"value"}, mutationToken=null}
....

If the same code is called again, a `DocumentAlreadyExistsException` will be thrown.
If you don't care that the document is overridden, you can use [.api]`upsert` instead:

[source,java]
----
JsonDocument doc = JsonDocument.create("document_id", JsonObject.empty().put("some", "other value"));
System.out.println(bucket.upsert(doc));
----

....
Output: JsonDocument{id='document_id', cas=216109392920576, expiry=0, content={"some":"other value"}, mutationToken=null}
....

Finally, a full document can be replaced if it existed before.
If it didn't exist, then a `DocumentDoesNotExistException` will be thrown:

[source,java]
----
JsonDocument doc = JsonDocument.create("document_id", JsonObject.empty().put("more", "content"));
System.out.println(bucket.replace(doc));
----

....
Output: JsonDocument{id='document_id', cas=216109395083264, expiry=0, content={"more":"content"}, mutationToken=null}
....

[#java-retrieving-full-docs]
== Retrieving full documents

You can retrieve documents using the [.api]`Bucket#get()`, [.api]`Bucket#getAndLock()`, [.api]`Bucket#getAndTouch()` and [.api]``Bucket#getFromReplica()``methods.
All of those serve different distinct purposes and accept different parameters.

Most of the time you use the [.api]`get()` method.
It accepts one mandatory argument:

* [.param]`id`: The document ID to retrieve

[source,java]
----
System.out.println(bucket.get("document_id"));
----

....
Output: JsonDocument{id='document_id', cas=216109395083264, expiry=0, content={"more":"content"}, mutationToken=null}
....

Other overloads are available for advanced purposes:

* [.param]`document`: Instead of just passing an Id a full document can be passed in.
If so, the ID is extracted and used.
* [.param]`target`: A custom Document type (other than `JsonDocument`) can be specified.
* [.param]`timeout`, [.param]`timeUnit`: Specify a custom timeout which overrides the default timeout setting.

[source,java]
----
// Use a Document where ID is extracted
JsonDocument someDoc = JsonDocument.create("document_id");
System.out.println(bucket.get(someDoc));
----

....
Output: JsonDocument{id='document_id', cas=216109395083264, expiry=0, content={"more":"content"}, mutationToken=null}
....

[source,java]
----
// A custom Document type, here it returns the plain raw JSON String, encoded.
RawJsonDocument doc = bucket.get("document_id", RawJsonDocument.class);
String content = doc.content();
System.out.println(content);
----

....
Output: {"more":"content"}
....

[source,java]
----
// Wait only 1 second instead of the default timeout
JsonDocument doc = bucket.get("document_id", 1, TimeUnit.SECONDS);
----

It is also possible to read from a replica if you want to explicitly trade availability for consistency during the timeframe when the active partition is not reachable (for example during a node failure or netsplit).

`getFromReplica` has one mandatory argument as well:

* [.param]`id`: The document ID to retrieve

Since you can have 0 to 3 replicas (and they can change at runtime of your application) the `getFromReplica` returns Lists or Iterators.
It is recommended to use the Iterator APIs since they provide more flexibility during error conditions (since only partial responses may be retreived).

[source,java]
----
Iterator<JsonDocument> docIter = bucket.getFromReplica("document_id");
while(docIter.hasNext()) {
    JsonDocument replicaDoc = docIter.next();
    System.out.println(replicaDoc);
}
----

Other overloads are available for advanced purposes:

* [.param]`replicaMode`: Allows to configure from which replicas to read from (defaults to all).
* [.param]`document`: Instead of just passing an Id a full document can be passed in.
If so, the ID is extracted and used.
* [.param]`target`: A custom Document type (other than `JsonDocument`) can be specified.
* [.param]`timeout`, [.param]`timeUnit`: Specify a custom timeout which overrides the default timeout setting.

TIP: In general, always use the [.param]`ReplicaMode.ALL` option and not [.param]`ReplicaMode.FIRST` and similar to just get the first replica.
The reason is that is that ALL will also try the active node, leading to more reliable behavior during failover.
If you just need the first replica use the iterator approach and `break;` once you have enough data from the replicas.

IMPORTANT: Since a replica is updated asynchronously and eventually consistent, reading from it may return stale and/or outdated results!

If you need to use pessimistic write locking on a document you can use the [.param]`getAndLock` which will retreive the document if it exists and also return its [.param]`CAS` value.
You need to provide a time that the document is maximum locked (and the server will unlock it then) if you don't update it with the valid cas.
Also note that this is a pure write lock, reading is still allowed.

[source,java]
----
// Get and Lock for max of 10 seconds
JsonDocument ownedDoc = bucket.getAndLock("document_id", 10);

// Do something with your document
JsonDocument modifiedDoc = modifyDocument(ownedDoc);

// Write it back with the correct CAS
bucket.replace(modifiedDoc);
----

If the document is locked already and you are trying to lock it again you will receive a `TemporaryLockFailureException`.

It is also possible to fetch the document and reset its expiration value at the same time.
See xref:document-operations.adoc#java-modifying-expiration[Modifying Expiration] for more information.

[#java-removing-full-docs]
== Removing full documents

You can remove documents using the [.api]`Bucket.remove()` method.
This method takes a single mandatory argument:

* [.param]`id`: The ID of the document to remove.

Some additional options:

* [.param]`persistTo`, [.param]`replicateTo`: Specify xref:durability.adoc[durability requirements] for the operations.
* [.param]`timeout`, [.param]`timeUnit`: Specify a custom timeout which overrides the default timeout setting.

If the `cas` value is set on the Document overload, it is used to provide optimistic currency, very much like the `replace` operation.

[source,java]
----
// Remove the document
JsonDocument removed = bucket.remove("document_id");
----

[source,java]
----
JsonDocument loaded = bucket.get("document_id");

// Remove and take the CAS into account
JsonDocument removed = bucket.remove(loaded);
----

[#java-modifying-expiration]
== Modifying expiration

Many methods support setting the expiry value as part of their other primary operations:

* [.api]`Bucket#touch`: Resets the expiry time for the given document ID to the value provided.
* [.api]`Bucket#getAndTouch`: Fetches the document and resets the expiry to the given value provided.
* [.api]`Bucket#insert`, [.api]`Bucket#upsert`, [.api]`Bucket#replace`: Stores the expiry value alongside the actual mutation when set on the `Document` instance.

The following example stores a document with an expiry, waits a bit longer and as a result no document is found on the subsequent get:

[source,java]
----
int expiry = 2; // seconds
JsonDocument stored = bucket.upsert(
    JsonDocument.create("expires", expiry, JsonObject.create().put("some", "value"))
);

Thread.sleep(3000);

System.out.println(bucket.get("expires"));
----

....
null
....

You may also use the [.api]`Bucket#touch()` method to modify expiration without fetching or modifying the document:

----
bucket.touch("expires", 2);
----

[#java-atomic-modifications]
== Atomic Document Modifications

Additional atomic document modifications can be performed using the Java SDK.
You can modify a xref:core-operations.adoc#devguide_kvcore_counter_generic[counter document] using the [.api]`Bucket.counter()` method.
You can also use the [.api]`Bucket.append()` and [.api]`Bucket.prepend()` methods to perform xref:core-operations.adoc#devguide_kvcore_append_prepend_generic[raw byte concatenation].

[#java-batching-ops]
== Batching Operations

Since the Java SDK uses RxJava as its asynchronous foundation, all operations can be xref:batching-operations.adoc[batched] in the SDK using the xref:async-programming.adoc[asynchronous API] via `bucket.async()` (and optionally revert back to blocking).

For implicit batching use these operators: `Observable.just()` or `Observable.from()` to generate an observable that contains the data you want to batch on.
`flatMap()` to send those events against the Couchbase Java SDK and merge the results asynchronously.
`last()` if you want to wait until the last element of the batch is received.
`toList()` if you care about the responses and want to aggregate them easily.
If you have more than one subscriber, use `cache()` to prevent accessing the network over and over again with every subscribe.

The following example creates an observable stream of 6 keys to load in a batch, asynchronously fires off `get()` requests against the SDK (notice the `+bucket.async().get(...)+`), waits until the last result has arrived, and then converts the result into a list and blocks at the very end.
This pattern can be reused for mutations like `upsert` (as shown further down):

[source,java]
----
Cluster cluster = CouchbaseCluster.create();
Bucket bucket = cluster.openBucket();

List<JsonDocument> foundDocs = Observable
    .just("key1", "key2", "key3", "key4", "inexistentDoc", "key5")
    .flatMap(new Func1<String, Observable<JsonDocument>>() {
        @Override
        public Observable<JsonDocument> call(String id) {
            return bucket.async().get(id);
        }
    })
    .toList()
    .toBlocking()
    .single();

for (JsonDocument doc : foundDocs) {
    System.out.println(doc.id());
}
----

....
key1
key2
key3
key4
key5
....

Note that this always returns a list, but it may contain 0 to 6 documents (here 5) depending on how many are actually found.
Also, at the very end the observable is converted into a blocking one, but everything before that, including the network calls and the aggregation, is happening completely asynchronously.

Inside the SDK, this provides much more efficient resource utilization because the requests are very quickly stored in the internal Request RingBuffer and the I/O threads are able to pick batches as large as they can.
Afterward, whatever server returns a result first it is stored in the list, so there is no serialization of responses going on.

Batching mutations: The previous Java SDK only provided bulk operations for get().
With the techniques shown above, you can perform any kind of operation as a batch operation.
The following code generates a number of fake documents and inserts them in one batch.
Note that you can decide to either collect the results with `toList()` as shown above or just use `last()` as shown here to wait until the last document is properly inserted:

[source,java]
----
// Generate a number of dummy JSON documents
int docsToCreate = 100;
List<JsonDocument> documents = new ArrayList<JsonDocument>();
for (int i = 0; i < docsToCreate; i++) {
    JsonObject content = JsonObject.create()
        .put("counter", i)
        .put("name", "Foo Bar");
    documents.add(JsonDocument.create("doc-"+i, content));
}

// Insert them in one batch, waiting until the last one is done.
Observable
    .from(documents)
    .flatMap(new Func1<JsonDocument, Observable<JsonDocument>>() {
        @Override
        public Observable<JsonDocument> call(final JsonDocument docToInsert) {
            return bucket.async().insert(docToInsert);
        }
    })
    .last()
    .toBlocking()
    .single();
----

[#java-subdocs]
== Operating with Sub-Documents

TIP: Sub-Document API is available starting Couchbase Server version 4.5.
See xref:subdocument-operations.adoc[Sub-Document Operations] for an overview.

Sub-document operations save network bandwidth by allowing you to specify _paths_ of a document to be retrieved or updated.
The document is parsed on the server and only the relevant sections (indicated by _paths_) are transferred between client and server.
You can execute xref:subdocument-operations.adoc[sub-document] operations in the Java SDK using the [.api]`Bucket#lookupIn()` and [.api]`Bucket#mutateIn()` methods.

Each of these methods accepts a [.param]`key` as its mandatory first argument and give you a builder that you can use to chain several _command specifications_, each specifying the path to be impacted by the specified operation and a document field operand.
You may find all the operations in the [.api]`LookupInBuilder` and [.api]`MutateInBuilder` classes.

[source,java]
----
bucket.lookupIn("docid")
    .get("path.to.get")
    .exists("check.path.exists")
    .execute();

boolean createParents = true;
bucket.mutateIn("docid")
    .upsert("path.to.upsert", value, createParents)
    .remove("path.to.del"))
    .execute();
----

All sub-document operations return a special [.api]`DocumentFragment` object rather than a [.api]`Document`.
It shares the `id()`, `cas()` and `mutationToken()` fields of a document, but in contrast with a normal [.api]`Document` object, a [.api]`DocumentFragment` object contains multiple results with multiple statuses, one result/status pair for every input operation.
So it exposes method to get the `content()` and `status()` of each spec, either by index or by path.
It also allows to check that a response for a particular spec `exists()`:

[source,java]
----
DocumentFragment<Lookup> res =
bucket.lookupIn("docid")
    .get("foo")
    .exists("bar")
    .exists("baz")
    .execute();

// First result
res.content("foo");
// or
res.content(0);
----

Using the `+content(...)+` methods will raise an exception if the individual spec did not complete successfully.
You can also use the `+status(...)+` methods to return an error code (a [.api]`ResponseStatus`) rather than throw an exception.

[#java-formats-non-json]
== Formats and Non-JSON Documents

TIP: See xref:nonjson.adoc[Non-JSON Documents] for a general overview of using non-JSON documents with Couchbase

The Java SDK defines several concrete implementations of a [.api]`Document` to represent the various data types that it can store.
Here is the complete list of document types:

.Documents with JSON content
|===
| Document Name | Description

| [.api]`JsonDocument`
| The default, which has a [.api]`JsonObject` at the top level content.

| [.api]`RawJsonDocument`
| Stores any JSON value and should be used if custom JSON serializers such as Jackson or GSON are already in use.

| [.api]`JsonArrayDocument`
| Similar to JsonDocument, but has a [.api]`JsonArray` at the top level content.

| [.api]`JsonBooleanDocument`
| Stores JSON-compatible Boolean values.

| [.api]`JsonLongDocument`
| Stores JSON compatible long (number) values.

| [.api]`JsonDoubleDocument`
| Stores JSON compatible double (number) values.

| [.api]`JsonStringDocument`
| Stores JSON compatible [.api]`String` values.
Input is automatically wrapped with quotes when stored.

| [.api]`EntityDocument`
| Used with the [.api]`Repository` implementation to write and read POJOs into JSON and back.
|===

.Documents with other content
|===
| Document Name | Description

| [.api]`BinaryDocument`
| Can be used to store arbitrary binary data.

| [.api]`SerializableDocument`
| Stores objects that implement [.api]`Serializable` through default Java object serialization.

| [.api]`LegacyDocument`
| Uses the [.api]`Transcoder` from the 1.x SDKs and can be used for full cross-compatibility between the old and new versions.

| [.api]`StringDocument`
| Can be used to store arbitrary strings.
They will not be quoted, but stored as-is and flagged as "String".
|===

You can implement a custom document type and associated transcoder if none of the pre-configured options are suitable for your application.
A custom transcoder converts intputs to their serialized forms, and deserializes encoded data based on the item flags.
There is an [.api]`AbstractTranscoder` that can serve as the basis for a custom implementation, and custom transcoders should be registered with a [.api]`Bucket` when calling [.api]`Cluster#openBucket` (a list of custom transcoders can be passed in one of the overloads).

[#java-binary-document]
== Correctly Managing BinaryDocuments

The `BinaryDocument` can be used to store and read arbitrary bytes.
It is the only default codec that directly exposes the underlying low-level Netty `ByteBuf` objects.

IMPORTANT: Because the raw data is exposed, it is important to free it after it has been properly used.
Not freeing it will result in increased garbage collection and memory leaks and should be avoided by all means.
See <<binary-memory>>.

Because binary data is arbitrary anyway, it is backward compatible with the old SDK regarding flags so that it can be read and written back and forth.
Make sure it is not compressed in the old SDK and that the same encoding and decoding process is used on the application side to avoid data corruption.

Here is some demo code that shows how to write and read raw data.
The example writes binary data, reads it back, and then frees the pooled resources:

[source,java]
----
// Create buffer out of a string
ByteBuf toWrite = Unpooled.copiedBuffer("Hello World", CharsetUtil.UTF_8);

// Write it
bucket.upsert(BinaryDocument.create("binaryDoc", toWrite));

// Read it back
BinaryDocument read = bucket.get("binaryDoc", BinaryDocument.class);

// Print it
System.out.println(read.content().toString(CharsetUtil.UTF_8));

// Free the resources
ReferenceCountUtil.release(read.content());
----

[#binary-memory]
== Correctly Managing Buffers

`BinaryDocument` allows users to get the rawest form of data out of Couchbase.
It  exposes Netty's `ByteBuf`, byte buffers that can have various characteristics (on- or off-heap, pooled or unpooled).
In general, buffers created by the SDK are pooled and off heap.
You can disable the pooling in the `CouchbaseEnvironment` if you absolutely need that.

As a consequence, the memory associated with the ByteBuf must be a little bit more managed by the developer than usual in Java.

Most notably, these byte buffers are reference counted, and you need to know three main methods associated to buffer management:

* `refCnt()` gives you the current reference count.
When it hits 0, the buffer is released back to its original pool, and it cannot be used anymore.
* `release()` will decrease the reference count by 1 (by default).
* `retain()` is the inverse of release, allowing you to prepare for multiple consumptions by external methods that you know will each release the buffer.

You can also use `ReferenceCountUtil.release(something)` if you don't want to check if `something` is actually a `ByteBuf` (will do nothing if it's not something that is [.api]`ReferenceCounted`).

IMPORTANT: The SDK bundles the Netty dependency into a different package so that it doesn't clash with a dependency to another version of Netty you may have.
As such, you need to use the classes and packages provided by the SDK (`com.couchbase.client.deps.io.netty`) when interacting with the API.
For example, the `ByteBuf` for the content of a `BinaryDocument` is a `com.couchbase.client.deps.io.netty.buffer.ByteBuf`.

*What happens if I don't release?*

Basically, you leak memory\... Netty will by default inspect a small percentage of `ByteBuf` creations and usage to try and detect leaks (in which case it will output a log, look for the "LEAK" keyword).

You can tune that to be more eagerly monitoring all buffers by calling `ResourceLeakDetector.setLevel(PARANOID)`.

IMPORTANT: Note that this incurs quite an overhead and should only be activated in tests.
In production (prod), setting it to `ADVANCED` is not as heavy as paranoid and can be a good middle ground.

*What happens if I release twice (or the SDK releases once more after I do)?*

Netty will throw `IllegalReferenceCountException`.
The buffer that has RefCnt = 0 cannot be interacted with anymore since it means it has been freed back into the pool.

*When must I release?*

When the SDK creates a `BinaryDocument` for you, basically GET-type operations.

Mutative operations, on the other hand, will take care of the buffer you pass in for you, at the time the buffer is written on the wire.

*When must I usually retain?*

When you do a write, the buffer will usually be released by the SDK calling `release()`.
But if you implement a kind of fallback behavior (for instance attempt to `insert()` a doc, catch `DocumentAlreadyExistException` and then fallback to an `update()` instead), that means the SDK would attempt to release twice, which won't work.

In this case you can `retain()` the buffer before the first attempt, let the catch block do the extra release if something goes wrong.
You have to manage the extra release if the first write succeeds, and think about catching other possible exceptions (here also an extra release is needed):

[source,java]
----
byteBuffer.retain(); //prepare for potential multi usage (+1 refCnt, refCnt = 2)
try {
   bucket.append(document);
   // refCnt = 2 on success
   byteBuffer.release(); //refCnt = 1
} catch (DocumentDoesNotExistException dneException) {
   // buffer is released on errors, refCnt = 1
   //second usage will also release, but we want to be at refCnt = 1 for the finally block
   byteBuffer.retain(); //refCnt = 2
   bucket.insert(document); //refCnt = 1
} // other uncaught errors will still cause refCnt to be released down to 1
finally {
   //we made sure that at this point refCnt = 1 in any case (success, caught exception, uncaught exception)
   byteBuffer.release(); //refCnt = 0, returned to the pool
}
----


